{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4168a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Taken from the EDA notebook\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "def get_filenames(mypath):\n",
    "    return [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"../data/audioset_v1_embeddings/bal_train/\" + i for i in get_filenames(\"../data/audioset_v1_embeddings/bal_train/\")]\n",
    "train_dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "# Make Pandas DataFrame out of the training set records.\n",
    "trainFeatures = pd.DataFrame(columns = ['video_id', 'time_stamp'].append(range(0,127)))\n",
    "for raw_record in train_dataset:\n",
    "    example = tf.train.SequenceExample()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    for i in range(0,len(example.feature_lists.feature_list['audio_embedding'].feature)):\n",
    "        vID = example.context.feature['video_id'].bytes_list.value[0]\n",
    "        time = example.context.feature['start_time_seconds'].float_list.value[0] + 0.96*i\n",
    "        newEntry = pd.DataFrame([vID, time].append(list(example.feature_lists.feature_list['audio_embedding'].feature[i].bytes_list.value[0])[j]), columns = ['video_id', 'time_stamp'].append(range(0,127)))\n",
    "        trainFeatures = pd.concat([trainFeatures, newEntry])\n",
    "\n",
    "trainFeatures.to_csv('trainFeatures.csv')\n",
    "\n",
    "# Respectively, speech, male speech, female speech, child speech, conversation, and narration.\n",
    "speech_events = set(['/m/09x0r', '/m/05zppz','/m/02zsn','/m/0ytgt','/m/01h8n0','/m/02qldy'])\n",
    "\n",
    "# Make a Pandas DataFrame of all instances of speech present.\n",
    "trainEvents = pd.read_csv(\"../data/audioset_train_strong.tsv\", sep=\"\\t\")\n",
    "for i in range(0,len(trainEvents)):\n",
    "    ''' Have to make the labels match the feature set,\n",
    "        and these labels have a \"_\" followed by trailing digits.'''\n",
    "    trainEvents[i,0] = trainEvents[i,0].rstrip(\"0123456789\")\n",
    "    trainEvents[i,0] = trainEvents[i,0].rstrip(\"_\")\n",
    "    if (trainEvents.loc[i,3] in speech_events) == False:\n",
    "        trainEvents.loc[i,3] = None\n",
    "trainEvents = trainEvents.dropna() # Deletes all rows with a None in it, i.e. entries that have no speech.\n",
    "\n",
    "# Finally, make a Pandas DF for the target: whether or not speech is present in each 0.96 second chunk.\n",
    "trainTargets = trainFeatures[:,:'time_stamp']\n",
    "del TrainFeatures # For RAM's sake. We've already saved this to a CSV.\n",
    "\n",
    "trainTargets['speech_present'] = False # By default.\n",
    "\n",
    "# Now check to see if each 0.96 second segment contains speech according to the trainEvents DF.\n",
    "'''This seems complicated at first glance, but the idea behind it is simple.\n",
    "Since each clip's events are grouped together in the trainEvents TSV, we just\n",
    "need to run a search ONCE for each clip label. Once we have it, we don't need\n",
    "to search again for the next entry's events unless its label is different.\n",
    "Sadly, the labels are NOT in alphabetical order, making an approach like this\n",
    "necessary.'''\n",
    "\n",
    "first_label_match = 0\n",
    "for i in range(0,len(trainTargets)):\n",
    "    if first_label_match == 0:\n",
    "        while (trainTargets.iloc[i,0] != trainEvents.iloc[first_label_match,0]):\n",
    "            first_label_match++\n",
    "    offset = 0\n",
    "    while (trainTargets.iloc[i,0] == trainEvents.iloc[first_label_match + offset,0]):\n",
    "        if trainTargets.iloc[i,1] <= trainEvents.iloc[first_label_match + offset,1]:\n",
    "            if trainTargets.iloc[i,1] + 0.96 >= trainEvents[first_label_match + offset,1]:\n",
    "                trainTargets.iloc[i,2] = True\n",
    "        if trainTargets.iloc[i,1] >= trainEvents.iloc[first_label_match + offset,1]:\n",
    "            if trainTargets.iloc[i,1] <= trainEvents.iloc[first_label_match + offset,2]:\n",
    "                trainTargets.iloc[i,2] = True\n",
    "        offset++\n",
    "    if i != len(trainTargets) - 1:\n",
    "        if trainTargets.loc[i,0] != trainTargets.loc[i+1,0]:\n",
    "            first_label_match = 0\n",
    "\n",
    "\n",
    "trainTargets.to_csv('trainTargets.csv')\n",
    "\n",
    "# When done, we can drop the labels and time indices, since the orders are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the eval DS...\n",
    "filenames = [\"../data/audioset_v1_embeddings/eval/\" + i for i in get_filenames(\"../data/audioset_v1_embeddings/bal_train/\")]\n",
    "eval_dataset = tf.data.TFRecordDataset(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d54fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was copy-pasted from my old personal project.\n",
    "# Some of the layers are not applicable (normalization),\n",
    "# and some layers (input, reshape, resizing, and dense) need to be resized.\n",
    "# shape = (,), (leftover from the input layer)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Input(ragged=True),\n",
    "    layers.Conv2D(4, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(4, 3, activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(200, activation='relu'),\n",
    "    layers.Dense(160, activation='relu'),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(80, activation='relu'),\n",
    "    layers.Dense(37, activation='relu')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mean_squared_error'])\n",
    "model.fit(trainFeatures,trainTargets,epochs = 100, batch_size = 4)\n",
    "if save_to_disk:\n",
    "    # Saving model to JSON and weights to H5.\n",
    "    os.chdir('..')\n",
    "    os.chdir('./models/')\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "loss  = model.evaluate(ValFeatures, ValTargets)\n",
    "print('Loss on Validation Set: ', loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
